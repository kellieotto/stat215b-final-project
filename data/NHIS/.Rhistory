#   dist: Range of possible K values. Should be concentrated about K=1.
#   alpha.hat: Estimated constant (mean of training set y).
#   B.hat: LS estimates of Beta given training set.
#   X.test: nxp matrix of x's to form test set.
#   y.test: length-n numeric vector of y's to form test set.
#
# Returns:
#   K value within distribution that minimizes MSPE.
# Estimate y for all possible K values
y.tilde.star <- lapply(dist, function(k) sapply(1:n,function(i) alpha.hat + k*t(B.hat) %*% X.test[i,]))
# Calculate MSPE for all y's
MSPE.K.star <- lapply(1:length(y.tilde.star), function(i) mean((y.test - y.tilde.star[[i]])^2) )
# Select K star with lowest MSPE
K.star <- dist[[which.min(MSPE.K.star)]]
return(K.star)
}
# Calculate optimal K value
K.star.dist <- seq(0.8,1,by=0.001) # create vector of possible K values
K.star <- GetKStar(K.star.dist, alpha.hat, B.hat, X.test, y.test)
# Use K.star to improve predictions
y.tilde.cv <- sapply(1:n,function(i) alpha.hat + K.star*t(B.hat) %*% X.test[i,])
# Calculate MSPE for K selected by CV
MSPE.cv <- mean((y.test - y.tilde.cv)^2)
## Statement 3
# Now, test set is nxp matrix of x's drawn from Beta distribution
X.test.beta <- matrix(rbeta(n*p,1,2), n)
y.test.beta <- sapply(1:n,function(i) rnorm(1,alpha + t(B) %*% X.test.beta[i,], sigma2))
# Get new K value
K.star.beta <- GetKStar(K.star.dist, alpha.hat, B.hat, X.test.beta, y.test.beta)
# Use new K to improve predictions
y.tilde.cv.beta <- sapply(1:n,function(i) alpha.hat + K.star.beta*t(B.hat) %*% X.test.beta[i,])
# Calculate MSPE for K selected by CV using new test set
MSPE.cv.beta <- mean((y.test.beta - y.tilde.cv.beta)^2)
return(list("Statement 1"= MSPE.K < MSPE.LS,
"Statement 2"= MSPE.K < MSPE.cv,
"Statement 3"= MSPE.K > MSPE.cv.beta,
"K.hat" = K.hat, # Copas formula (3.7)
"K.star" = K.star, # CV
"K.star.beta" = K.star.beta, # CV with Weibull dist. test set
"MSPE.LS" = MSPE.LS,
"MSPE.K" = MSPE.K,
"MSPE.cv" = MSPE.cv,
"MSPE.cv.beta" = MSPE.cv.beta))
}
# Define parameters
n <- 100 # training set size
p <- 10 # no. predictive factors
alpha <- 12 # constant
sigma2 <- 2.4 # variance
B <- runif(p,-1,1)  # length-p vector of Betas
# Call EvalMSPE for L simulation runs
L <- 100
sim.result <- foreach(i = 1:L, .combine = 'rbind') %dopar% {
data.list <- EvalMSPE(n,p,alpha,sigma2,B)
return(data.frame(data.list))
}
sim.result$Replicate <- 1:L
# Plot comparisons for each statement
MSPE.melt <- melt(sim.result[c("Replicate","MSPE.LS","MSPE.K","MSPE.cv","MSPE.cv.beta")],
id.vars="Replicate",value.name="MSPE",variable.name="Type")
# Statement 1
bp.1 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.LS",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("orange", "blue")) +
ylim(1,2.5) +
ylab("Mean squared prediction error (MSPE)") +
scale_x_discrete("", labels = c("LS","Copas K"))
bp.2 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "green")) +
ylim(1,2.5) +
ylab("") +
scale_x_discrete("Method", labels = c("Copas K", "CV K"))
bp.3 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv.beta",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "pink")) +
ylim(1,2.5) +
ylab("") +
scale_x_discrete("", labels = c("Copas K", "CV K with Beta distribution"))
# Combine plots
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
print(bp.1, vp = vplayout(1, 1))
print(bp.2, vp=vplayout(1,2))
print(bp.3, vp=vplayout(1,3))
# Statement 1
bp.1 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.LS",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("orange", "blue")) +
#  ylim(1,2.5) +
ylab("Mean squared prediction error (MSPE)") +
scale_x_discrete("", labels = c("LS","Copas K"))
bp.2 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "green")) +
#  ylim(1,2.5) +
ylab("") +
scale_x_discrete("Method", labels = c("Copas K", "CV K"))
bp.3 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv.beta",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "pink")) +
# ylim(1,2.5) +
ylab("") +
scale_x_discrete("", labels = c("Copas K", "CV K with Beta distribution"))
# Combine plots
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
print(bp.1, vp = vplayout(1, 1))
print(bp.2, vp=vplayout(1,2))
print(bp.3, vp=vplayout(1,3))
summary(sim.result)
# Statement 1
bp.1 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.LS",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("orange", "blue")) +
ylim(4,9) +
ylab("Mean squared prediction error (MSPE)") +
scale_x_discrete("", labels = c("LS","Copas K"))
bp.2 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "green")) +
ylim(4,9) +
ylab("") +
scale_x_discrete("Method", labels = c("Copas K", "CV K"))
bp.3 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv.beta",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "pink")) +
ylim(4,9) +
ylab("") +
scale_x_discrete("", labels = c("Copas K", "CV K with Beta distribution"))
# Combine plots
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
print(bp.1, vp = vplayout(1, 1))
print(bp.2, vp=vplayout(1,2))
print(bp.3, vp=vplayout(1,3))
View(sim.result)
summary(sim.result)
######################
# Stat 215b - Assignment 6
# Jason Poulos
# Libraries
library(ggplot2)
library(MASS)
library(matrixStats)
library(foreach)
library(doParallel)
library(reshape2)
library(grid)
# Register cores for parallel processing
registerDoParallel(4)
# Ensure random number generation for parallel processing
RNGkind("L'Ecuyer-CMRG")
######################
EvalMSPE <-function(n,p,alpha,sigma2,B){
# Evaluates each three statements and returns the observed mean squared prediction errors (MSPE)
# for the relevant procedures.
#
# Args:
#   n: Training set size.
#   p: Number of predictive factors.
#   alpha: Constant.
#   sigma2: Variance.
#   B: Length-p vector of Betas.
#
# Returns:
#   Vectors containing logical statements evaluating each statement;
#   vectors of optimal K's;
#   vector of observed MSPEs.
# Training set is nxp matrix of x's fixed and centered around sample means
X.train <- matrix(rexp(n*p), n) # draw from exponential dist.
X.train <- scale(X.train, center = TRUE, scale = FALSE) # center columns
# Y is length-n vector of iid N(0,1) draws of y
y.train <- sapply(1:n,function(i) rnorm(1,alpha + t(B) %*% X.train[i,], sigma2))
alpha.hat <- mean(y.train)
# LS estimates of Beta given training set
B.hat <- solve(t(X.train) %*% X.train) %*% t(X.train) %*% y.train
# Create variance-covariance matrix
V <- n**(-1)*t(X.train) %*% X.train
# Calculate residual mean square (MSE)
y.hat.train <- sapply(1:n,function(i) alpha.hat + t(B.hat) %*% X.train[i,])
sigma2.hat <- mean((y.train - y.hat.train)^2)
# Calculate distribution of K
delta2 <- sigma2/(n*t(B) %*% V %*% B)
mu <- rnorm(n)
xi <- rchisq(n,p-1)
K.dist <- (1+mu*sqrt(delta2))/((1+mu*sqrt(delta2))^2 + xi*delta2)
# Estimate K.hat
k <- min(K.dist) # pick k value within K distribution
F <- (n*t(B.hat) %*% V %*% B.hat)/(p*sigma2.hat) # calculate F ratio
K.hat <- ((F - p**(-1)*k)/(F))[[1]] # 3.7
## Statement 1
# Test set is nxp matrix of x's drawn from multivariate normal distribution, N(0,V)
X.test <- mvrnorm(n, rep(0,p), V)
y.test <- sapply(1:n,function(i) rnorm(1,alpha + t(B) %*% X.test[i,], sigma2)) # y's gen. from reg. model
# LS predictor
y.hat.test <- sapply(1:n,function(i) alpha.hat + t(B.hat) %*% X.test[i,]) # K=1 for LS predictor
# Use K.hat(k) to improve predictions
y.tilde <- sapply(1:n,function(i) alpha.hat + K.hat*t(B.hat) %*% X.test[i,])
# Calculate MSPEs for LS predictor and using K formula
MSPE.LS <- mean((y.test - y.hat.test)^2)
MSPE.K <- mean((y.test - y.tilde)^2)
## Statement 2
GetKStar <-function(dist, alpha.hat, B.hat, X.test, y.test){
# Returns K with lowest MSPE from distribution of possible K values.
#
# Args:
#   dist: Range of possible K values. Should be concentrated about K=1.
#   alpha.hat: Estimated constant (mean of training set y).
#   B.hat: LS estimates of Beta given training set.
#   X.test: nxp matrix of x's to form test set.
#   y.test: length-n numeric vector of y's to form test set.
#
# Returns:
#   K value within distribution that minimizes MSPE.
# Estimate y for all possible K values
y.tilde.star <- lapply(dist, function(k) sapply(1:n,function(i) alpha.hat + k*t(B.hat) %*% X.test[i,]))
# Calculate MSPE for all y's
MSPE.K.star <- lapply(1:length(y.tilde.star), function(i) mean((y.test - y.tilde.star[[i]])^2) )
# Select K star with lowest MSPE
K.star <- dist[[which.min(MSPE.K.star)]]
return(K.star)
}
# Calculate optimal K value
K.star.dist <- seq(0.2,1,by=0.01) # create vector of possible K values
K.star <- GetKStar(K.star.dist, alpha.hat, B.hat, X.test, y.test)
# Use K.star to improve predictions
y.tilde.cv <- sapply(1:n,function(i) alpha.hat + K.star*t(B.hat) %*% X.test[i,])
# Calculate MSPE for K selected by CV
MSPE.cv <- mean((y.test - y.tilde.cv)^2)
## Statement 3
# Now, test set is nxp matrix of x's drawn from Beta distribution
X.test.beta <- matrix(rbeta(n*p,1,2), n)
y.test.beta <- sapply(1:n,function(i) rnorm(1,alpha + t(B) %*% X.test.beta[i,], sigma2))
# Get new K value
K.star.beta <- GetKStar(K.star.dist, alpha.hat, B.hat, X.test.beta, y.test.beta)
# Use new K to improve predictions
y.tilde.cv.beta <- sapply(1:n,function(i) alpha.hat + K.star.beta*t(B.hat) %*% X.test.beta[i,])
# Calculate MSPE for K selected by CV using new test set
MSPE.cv.beta <- mean((y.test.beta - y.tilde.cv.beta)^2)
return(list("Statement 1"= MSPE.K < MSPE.LS,
"Statement 2"= MSPE.K < MSPE.cv,
"Statement 3"= MSPE.K > MSPE.cv.beta,
"K.hat" = K.hat, # Copas formula (3.7)
"K.star" = K.star, # CV
"K.star.beta" = K.star.beta, # CV with Weibull dist. test set
"MSPE.LS" = MSPE.LS,
"MSPE.K" = MSPE.K,
"MSPE.cv" = MSPE.cv,
"MSPE.cv.beta" = MSPE.cv.beta))
}
# Define parameters
n <- 100 # training set size
p <- 10 # no. predictive factors
alpha <- 12 # constant
sigma2 <- 2.4 # variance
B <- runif(p,-1,1)  # length-p vector of Betas
# Call EvalMSPE for L simulation runs
L <- 100
sim.result <- foreach(i = 1:L, .combine = 'rbind') %dopar% {
data.list <- EvalMSPE(n,p,alpha,sigma2,B)
return(data.frame(data.list))
}
sim.result$Replicate <- 1:L
# Plot comparisons for each statement
MSPE.melt <- melt(sim.result[c("Replicate","MSPE.LS","MSPE.K","MSPE.cv","MSPE.cv.beta")],
id.vars="Replicate",value.name="MSPE",variable.name="Type")
# Statement 1
bp.1 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.LS",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("orange", "blue")) +
ylim(4,9) +
ylab("Mean squared prediction error (MSPE)") +
scale_x_discrete("", labels = c("LS","Copas K"))
bp.2 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "green")) +
ylim(4,9) +
ylab("") +
scale_x_discrete("Method", labels = c("Copas K", "CV K"))
bp.3 <- ggplot(MSPE.melt[MSPE.melt$Type=="MSPE.K" | MSPE.melt$Type=="MSPE.cv.beta",], aes(factor(Type), MSPE)) +
geom_boxplot(aes(fill = factor(Type))) +
theme(legend.position="none") +
scale_fill_manual(values = c("blue", "pink")) +
ylim(4,9) +
ylab("") +
scale_x_discrete("", labels = c("Copas K", "CV K with Beta distribution"))
# Combine plots
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 3)))
print(bp.1, vp = vplayout(1, 1))
print(bp.2, vp=vplayout(1,2))
print(bp.3, vp=vplayout(1,3))
summary(sim.result)
sim.result$Statement.1
sum(sim.result$Statement.1)
sum(sim.result$Statement.1)/L
100/500
View(sim.result)
?fisher.test
View(sim.result)
?t.test
sim.result$Statement.1
class(sim.result$Statement.1)
t.test(as.numeric(sim.result$Statement.1))
?binom.test
as.numeric(sim.result$Statement.1)
sum(sim.result$Statement.1)
binom.test(sum(sim.result$Statement.1),L)
binom.test(sum(sim.result$Statement.1),L,alternative="two.sided")
as.numeric(sim.result$Statement.1)
wilcox.test(as.numeric(sim.result$Statement.1),alternative="two.sided")
wilcox.test(as.numeric(sim.result$Statement.2),alternative="two.sided")
wilcox.test(as.numeric(sim.result$Statement.3),alternative="two.sided")
as.numeric(sim.result$Statement.2)
as.numeric(sim.result$Statement.3)
wilcox.test(as.numeric(sim.result$Statement.1))
wilcox.test(as.numeric(sim.result$Statement.1))
wilcox.test(as.numeric(sim.result$Statement.2))
wilcox.test(as.numeric(sim.result$Statement.3))
View(sim.result)
as.numeric(sim.result$Statement.1)
mean(as.numeric(sim.result$Statement.1))
wilcox.test(as.numeric(sim.result$Statement.2))
(as.numeric(sim.result$Statement.2)
mean(as.numeric(sim.result$Statement.2))
mean(as.numeric(sim.result$Statement.3))
wilcox.test(as.numeric(sim.result$Statement.3))
?rnorm
View(sim.result)
save.image("~/Dropbox/stat215b/lab6/sim-results.RData")
## Prepares National Health Interview Survey (NHIS) data
# Libraries
library(foreach)
library(doParallel)
# Register cores for parallel processing
registerDoParallel(4)
# Set directory to NHIS data directory
setwd("~/Dropbox/github/stat215b-final-project/data/NHIS")
# Create function to run merge script for all years
# Script merges person with adult sample files and takes the average
# of imputed income vars across five imputed income files
nhisMerge <- function(i){
year <- i
source("merge-nhis.R", local=TRUE)
return(x.sa)
}
years <- c(2009:2013) # leave out 2008 for now -- imputed income is categorical
# Combine each merged dataset into list
nhis <- foreach(i=years) %dopar% {
data <- nhisMerge(i)
return(data)
}
names(nhis) <- c("2009","2010","2011","2012","2013") # name elements
summary(nhis[['2009']])
nhis[['2009']]$srvy_yr
nhis[['2010']]$srvy_yr
## Imports NHIS and OHIE datasets and creates outcome vectors and common covariates for the analysis
# Define directory for analysis
directory <- "~/Dropbox/github/stat215b-final-project/analysis"
# Source data prep scripts
source(file.path(directory,"prepare-ohie.R"))
source(file.path(directory,"prepare-nhis.R"))
colnames(descriptive)
colnames(descriptive)
## Prepares Oregon Health Insurance Experiment (OHIE) data
# Libraries
library(foreign)
# Define data directory
data.directory <- "~/Dropbox/github/stat215b-final-project/data/OHIE_Public_Use_Files/OHIE_Data"
# Import data
f <- file.path(data.directory, c("oregonhie_descriptive_vars.dta",
"oregonhie_ed_vars.dta",
"oregonhie_inperson_vars.dta",
"oregonhie_stateprograms_vars.dta",
"oregonhie_survey0m_vars.dta",
"oregonhie_survey12m_vars.dta"))
ohie <- lapply(f, read.dta) # read data to list
names(ohie) <- gsub(".*/oregonhie_(.*)\\..*", "\\1", f) # name elements
# Extract each element of list as a data frame
descriptive <- as.data.frame(ohie[["descriptive_vars"]])
ed <- as.data.frame(ohie[["ed_vars"]])
inperson <- as.data.frame(ohie[["inperson_vars"]])
stateprograms <- as.data.frame(ohie[["stateprograms_vars"]])
survey0m <- as.data.frame(ohie[["survey0m_vars"]])
survey12m <- as.data.frame(ohie[["survey12m_vars"]])
# Clean up workspace
rm(data.directory,f)
View(descriptive)
View(ed)
## Prepares Oregon Health Insurance Experiment (OHIE) data
# Libraries
library(foreign)
# Define data directory
data.directory <- "~/Dropbox/github/stat215b-final-project/data/OHIE_Public_Use_Files/OHIE_Data"
# Import data
f <- file.path(data.directory, c("oregonhie_descriptive_vars.dta",
"oregonhie_ed_vars.dta",
"oregonhie_inperson_vars.dta",
"oregonhie_stateprograms_vars.dta",
"oregonhie_survey0m_vars.dta",
"oregonhie_survey12m_vars.dta"))
ohie.list <- lapply(f, read.dta) # read data to list
names(ohie.list) <- gsub(".*/oregonhie_(.*)\\..*", "\\1", f) # name elements
# Merge into single dataframe by unique person ID
ohie <- Reduce(function(...) merge(..., by=person_id, all=T), ohie.list)
# Clean up workspace
rm(data.directory,f)
ohie <- Reduce(function(...) merge(..., by="person_id", all=T), ohie.list)
View(ohie)
summary(ohie.list)
nrow(ohie.list[['descriptive_vars']])
nrow(ohie.list[['ed_vars']])
nrow(ohie.list[['inperson_vars']])
## Imports NHIS and OHIE datasets and creates outcome vectors and common covariates for the analysis
# Define directory for analysis
directory <- "~/Dropbox/github/stat215b-final-project/analysis"
# Source data prep scripts
source(file.path(directory,"prepare-ohie.R"))
source(file.path(directory,"prepare-nhis.R"))
rm(ohie.list)
## Prepares Oregon Health Insurance Experiment (OHIE) data
# Libraries
library(foreign)
# Define data directory
data.directory <- "~/Dropbox/github/stat215b-final-project/data/OHIE_Public_Use_Files/OHIE_Data"
# Import data
f <- file.path(data.directory, c("oregonhie_descriptive_vars.dta",
"oregonhie_ed_vars.dta",
"oregonhie_inperson_vars.dta",
"oregonhie_stateprograms_vars.dta",
"oregonhie_survey0m_vars.dta",
"oregonhie_survey12m_vars.dta"))
ohie.list <- lapply(f, read.dta) # read data to list
names(ohie.list) <- gsub(".*/oregonhie_(.*)\\..*", "\\1", f) # name elements
# Merge into single dataframe by unique person ID
ohie <- Reduce(function(...) merge(..., by="person_id", all=T), ohie.list)
# Clean up workspace
rm(data.directory,f,ohie.list)
colnames(ohie)
treatment <- ohie$treatment
summary(treatment)
levels(treatment)
?SuperLearner
library(SuperLearner)
?SuperLearner
n.hh <- ohie$numhh_list
n.hh
levels(n.hh)
ohie$hhid
??count
summary(ohie$household_id)
??count
library(plyr)
count(ohie$household_id)
summary(count(ohie$household_id))
summary(n.hh)
levels(n.hh)
count(ohie$draw_lottery)
colnames(ohie)
insurance <- ohie$ohp_all_ever_matchn_30sep2009
insurance
count(insurance)
treatment==0
summary(treatment)
count(treatment==1)
count(treatment==0)
summary(treatment)
summary(as.numeric(treatment))
count(as.numeric(treatment))
library(weights)
dummify(treatment)
head(dummify(treatment))
levels(ohie$treatment)
treatment <- ifelse(ohie$treatment=="Selected",1,0)
summary(treatment)
count(treatment)
count(ohie$treatment)
ohie$numhh_list
count(ohie$numhh_list)
n.hh <- dummify(ohie$numhh_list)
View(n.hh)
count(insurance)
insurance <- ifelse(ohie$ohp_all_ever_matchn_30sep2009=="Enrolled",1,0)
count(insurance)
insurance[treatment] # c
sum(insurance[treatment==1]
)
tab(insurance,treatment)
table(insurance, treatment) # check for one-way crossover
table(ohie$treatment,ohie$ohp_all_ever_matchn_30sep2009)
table(ohie$treatment,ohie$ohp_all_ever_firstn_30sep2009)
table(ohie$treatment,ohie$ohp_all_ever_matchn_30sep2009)
table(insurance, treatment) # check for one-way crossover
# (variable used for analysis of hospital discharge data in Taubman et al. 2014)
insurance <- ifelse(ohie$ohp_all_ever_firstn_30sep2009=="Enrolled",1,0)
table(insurance, treatment) # check for one-way crossover
